{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from input_adapters import PatchedInputAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN_CONF = {\n",
    "    'rgb': {\n",
    "        'channels': 3,\n",
    "        'stride_level': 1,\n",
    "        'input_adapter': partial(PatchedInputAdapter, num_channels=3),\n",
    "        'aug_type': 'image',\n",
    "    },\n",
    "    'depth': {\n",
    "        'channels': 1,\n",
    "        'stride_level': 1,\n",
    "        'input_adapter': partial(PatchedInputAdapter, num_channels=1),\n",
    "        'aug_type': 'mask',\n",
    "    },\n",
    "    'mask_valid': {\n",
    "        'stride_level': 1,\n",
    "        'aug_type': 'mask',\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "in_domains = ['rgb']\n",
    "out_domains = ['depth']\n",
    "all_domains = list(set(in_domains) | set(out_domains))\n",
    "\n",
    "patch_size = 16\n",
    "input_size = 224\n",
    "\n",
    "input_adapters = {\n",
    "        domain: DOMAIN_CONF[domain]['input_adapter'](\n",
    "            stride_level=DOMAIN_CONF[domain]['stride_level'],\n",
    "            patch_size_full=patch_size,\n",
    "            image_size=input_size,\n",
    "        )\n",
    "        for domain in in_domains\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from output_adapters import DPTOutputAdapter, ConvNeXtAdapter\n",
    "\n",
    "decoder_main_tasks = ['rgb']\n",
    "\n",
    "additional_targets = {domain: DOMAIN_CONF[domain]['aug_type'] for domain in all_domains}\n",
    "\n",
    "# DPT settings are fixed for ViT-B. Modify them if using a different backbone.\n",
    "\n",
    "adapters_dict = {\n",
    "    'dpt': DPTOutputAdapter,\n",
    "    'convnext': partial(ConvNeXtAdapter, preds_per_patch=64),\n",
    "}\n",
    "\n",
    "output_adapter = 'dpt'\n",
    "\n",
    "output_adapters = {\n",
    "    domain: adapters_dict[output_adapter](\n",
    "        num_classes=DOMAIN_CONF[domain]['channels'],\n",
    "        stride_level=DOMAIN_CONF[domain]['stride_level'],\n",
    "        patch_size=patch_size,\n",
    "        main_tasks=decoder_main_tasks\n",
    "    )\n",
    "    for domain in out_domains\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimae import multivit_base\n",
    "\n",
    "model_name = 'multivit_base'\n",
    "drop_path_encoder = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model = multivit_base(input_adapters=input_adapters, output_adapters=output_adapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3, 224, 224)\n",
    "depth = torch.rand(5, 1, 224, 224)\n",
    "sample_dict = {'rgb': x}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.rand(5, 197, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(sample_dict, return_all_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['depth'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_berhu_loss(preds, target, mask_valid=None):\n",
    "    if mask_valid is None:\n",
    "        mask_valid = torch.ones_like(preds).bool()\n",
    "    if preds.shape[1] != mask_valid.shape[1]:\n",
    "        mask_valid = mask_valid.repeat_interleave(preds.shape[1], 1)\n",
    "\n",
    "    diff = preds - target\n",
    "    diff[~mask_valid] = 0\n",
    "    with torch.no_grad():\n",
    "        c = max(torch.abs(diff).max() * 0.2, 1e-5)\n",
    "\n",
    "    l1_loss = torch.abs(diff)\n",
    "    l2_loss = (torch.square(diff) + c**2) / 2. / c\n",
    "    berhu_loss = l1_loss[torch.abs(diff) < c].sum() + l2_loss[torch.abs(diff) >= c].sum()\n",
    "\n",
    "    return berhu_loss / mask_valid.sum()\n",
    "\n",
    "@torch.no_grad()\n",
    "def masked_metrics(preds, target, mask_valid=None):\n",
    "    # map to the original scale \n",
    "    # preds = preds * NYU_STD + NYU_MEAN\n",
    "    # target = target * NYU_STD + NYU_MEAN\n",
    "\n",
    "    if mask_valid is None:\n",
    "        mask_valid = torch.ones_like(preds).bool()\n",
    "    if preds.shape[1] != mask_valid.shape[1]:\n",
    "        mask_valid = mask_valid.repeat_interleave(preds.shape[1], 1)\n",
    "\n",
    "    n = mask_valid.sum()\n",
    "    \n",
    "    diff = torch.abs(preds - target)\n",
    "    diff[~mask_valid] = 0\n",
    "    \n",
    "    max_rel = torch.maximum(preds/torch.clamp_min(target, 1e-6), target/torch.clamp_min(preds, 1e-6))\n",
    "    max_rel = max_rel[mask_valid]\n",
    "\n",
    "    log_diff = torch.log(torch.clamp_min(preds, 1e-6)) - torch.log(torch.clamp_min(target, 1e-6))\n",
    "    log_diff[~mask_valid] = 0\n",
    "\n",
    "    metrics = {\n",
    "        'rmse': (diff.square().sum() / n).sqrt(),\n",
    "        'rel': (diff/torch.clamp_min(target, 1e-6))[mask_valid].mean(),\n",
    "        'srel': (diff**2/torch.clamp_min(target, 1e-6))[mask_valid].mean(),\n",
    "        'log10': (log_diff.square().sum() / n).sqrt(),\n",
    "        'delta_1': (max_rel < 1.25).float().mean(),\n",
    "        'delta_2': (max_rel < (1.25**2)).float().mean(),\n",
    "        'delta_3': (max_rel < (1.25**3)).float().mean(),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = masked_berhu_loss(preds=out['depth'], target=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = masked_metrics(preds=out['depth'], target=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rmse': tensor(0.5789),\n",
       " 'rel': tensor(1.0782),\n",
       " 'srel': tensor(0.5048),\n",
       " 'log10': tensor(10.3892),\n",
       " 'delta_1': tensor(0.0010),\n",
       " 'delta_2': tensor(0.0021),\n",
       " 'delta_3': tensor(0.0033)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pos_embed_multi import interpolate_pos_embed_multimae\n",
    "\n",
    "\n",
    "finetune_path = './data/mae-b_dec512d8b_1600e_multivit-c477195b.pth'\n",
    "checkpoint = torch.load(finetune_path, map_location='cpu')\n",
    "\n",
    "checkpoint_model = checkpoint['model']\n",
    "\n",
    "# # Remove keys for semantic segmentation\n",
    "# for k in list(checkpoint_model.keys()):\n",
    "#     if \"semseg\" in k:\n",
    "#         del checkpoint_model[k]\n",
    "\n",
    "\n",
    "# Remove output adapters\n",
    "for k in list(checkpoint_model.keys()):\n",
    "    if \"output_adapters\" in k:\n",
    "        del checkpoint_model[k]\n",
    "\n",
    "\n",
    "# Interpolate position embedding\n",
    "interpolate_pos_embed_multimae(model, checkpoint_model)\n",
    "\n",
    "# Load pre-trained model\n",
    "msg = model.load_state_dict(checkpoint_model, strict=False)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['output_adapters.depth.scratch.layer1_rn.weight', 'output_adapters.depth.scratch.layer2_rn.weight', 'output_adapters.depth.scratch.layer3_rn.weight', 'output_adapters.depth.scratch.layer4_rn.weight', 'output_adapters.depth.scratch.layer_rn.0.weight', 'output_adapters.depth.scratch.layer_rn.1.weight', 'output_adapters.depth.scratch.layer_rn.2.weight', 'output_adapters.depth.scratch.layer_rn.3.weight', 'output_adapters.depth.scratch.refinenet1.out_conv.weight', 'output_adapters.depth.scratch.refinenet1.out_conv.bias', 'output_adapters.depth.scratch.refinenet1.resConfUnit1.conv1.weight', 'output_adapters.depth.scratch.refinenet1.resConfUnit1.conv1.bias', 'output_adapters.depth.scratch.refinenet1.resConfUnit1.conv2.weight', 'output_adapters.depth.scratch.refinenet1.resConfUnit1.conv2.bias', 'output_adapters.depth.scratch.refinenet1.resConfUnit2.conv1.weight', 'output_adapters.depth.scratch.refinenet1.resConfUnit2.conv1.bias', 'output_adapters.depth.scratch.refinenet1.resConfUnit2.conv2.weight', 'output_adapters.depth.scratch.refinenet1.resConfUnit2.conv2.bias', 'output_adapters.depth.scratch.refinenet2.out_conv.weight', 'output_adapters.depth.scratch.refinenet2.out_conv.bias', 'output_adapters.depth.scratch.refinenet2.resConfUnit1.conv1.weight', 'output_adapters.depth.scratch.refinenet2.resConfUnit1.conv1.bias', 'output_adapters.depth.scratch.refinenet2.resConfUnit1.conv2.weight', 'output_adapters.depth.scratch.refinenet2.resConfUnit1.conv2.bias', 'output_adapters.depth.scratch.refinenet2.resConfUnit2.conv1.weight', 'output_adapters.depth.scratch.refinenet2.resConfUnit2.conv1.bias', 'output_adapters.depth.scratch.refinenet2.resConfUnit2.conv2.weight', 'output_adapters.depth.scratch.refinenet2.resConfUnit2.conv2.bias', 'output_adapters.depth.scratch.refinenet3.out_conv.weight', 'output_adapters.depth.scratch.refinenet3.out_conv.bias', 'output_adapters.depth.scratch.refinenet3.resConfUnit1.conv1.weight', 'output_adapters.depth.scratch.refinenet3.resConfUnit1.conv1.bias', 'output_adapters.depth.scratch.refinenet3.resConfUnit1.conv2.weight', 'output_adapters.depth.scratch.refinenet3.resConfUnit1.conv2.bias', 'output_adapters.depth.scratch.refinenet3.resConfUnit2.conv1.weight', 'output_adapters.depth.scratch.refinenet3.resConfUnit2.conv1.bias', 'output_adapters.depth.scratch.refinenet3.resConfUnit2.conv2.weight', 'output_adapters.depth.scratch.refinenet3.resConfUnit2.conv2.bias', 'output_adapters.depth.scratch.refinenet4.out_conv.weight', 'output_adapters.depth.scratch.refinenet4.out_conv.bias', 'output_adapters.depth.scratch.refinenet4.resConfUnit1.conv1.weight', 'output_adapters.depth.scratch.refinenet4.resConfUnit1.conv1.bias', 'output_adapters.depth.scratch.refinenet4.resConfUnit1.conv2.weight', 'output_adapters.depth.scratch.refinenet4.resConfUnit1.conv2.bias', 'output_adapters.depth.scratch.refinenet4.resConfUnit2.conv1.weight', 'output_adapters.depth.scratch.refinenet4.resConfUnit2.conv1.bias', 'output_adapters.depth.scratch.refinenet4.resConfUnit2.conv2.weight', 'output_adapters.depth.scratch.refinenet4.resConfUnit2.conv2.bias', 'output_adapters.depth.head.0.weight', 'output_adapters.depth.head.0.bias', 'output_adapters.depth.head.2.weight', 'output_adapters.depth.head.2.bias', 'output_adapters.depth.head.4.weight', 'output_adapters.depth.head.4.bias', 'output_adapters.depth.act_1_postprocess.0.weight', 'output_adapters.depth.act_1_postprocess.0.bias', 'output_adapters.depth.act_1_postprocess.1.weight', 'output_adapters.depth.act_1_postprocess.1.bias', 'output_adapters.depth.act_2_postprocess.0.weight', 'output_adapters.depth.act_2_postprocess.0.bias', 'output_adapters.depth.act_2_postprocess.1.weight', 'output_adapters.depth.act_2_postprocess.1.bias', 'output_adapters.depth.act_3_postprocess.0.weight', 'output_adapters.depth.act_3_postprocess.0.bias', 'output_adapters.depth.act_4_postprocess.0.weight', 'output_adapters.depth.act_4_postprocess.0.bias', 'output_adapters.depth.act_4_postprocess.1.weight', 'output_adapters.depth.act_4_postprocess.1.bias', 'output_adapters.depth.act_postprocess.0.0.weight', 'output_adapters.depth.act_postprocess.0.0.bias', 'output_adapters.depth.act_postprocess.0.1.weight', 'output_adapters.depth.act_postprocess.0.1.bias', 'output_adapters.depth.act_postprocess.1.0.weight', 'output_adapters.depth.act_postprocess.1.0.bias', 'output_adapters.depth.act_postprocess.1.1.weight', 'output_adapters.depth.act_postprocess.1.1.bias', 'output_adapters.depth.act_postprocess.2.0.weight', 'output_adapters.depth.act_postprocess.2.0.bias', 'output_adapters.depth.act_postprocess.3.0.weight', 'output_adapters.depth.act_postprocess.3.0.bias', 'output_adapters.depth.act_postprocess.3.1.weight', 'output_adapters.depth.act_postprocess.3.1.bias'], unexpected_keys=['decoder_encoder.0.norm1.weight', 'decoder_encoder.0.norm1.bias', 'decoder_encoder.0.attn.qkv.weight', 'decoder_encoder.0.attn.qkv.bias', 'decoder_encoder.0.attn.proj.weight', 'decoder_encoder.0.attn.proj.bias', 'decoder_encoder.0.norm2.weight', 'decoder_encoder.0.norm2.bias', 'decoder_encoder.0.mlp.fc1.weight', 'decoder_encoder.0.mlp.fc1.bias', 'decoder_encoder.0.mlp.fc2.weight', 'decoder_encoder.0.mlp.fc2.bias', 'decoder_encoder.1.norm1.weight', 'decoder_encoder.1.norm1.bias', 'decoder_encoder.1.attn.qkv.weight', 'decoder_encoder.1.attn.qkv.bias', 'decoder_encoder.1.attn.proj.weight', 'decoder_encoder.1.attn.proj.bias', 'decoder_encoder.1.norm2.weight', 'decoder_encoder.1.norm2.bias', 'decoder_encoder.1.mlp.fc1.weight', 'decoder_encoder.1.mlp.fc1.bias', 'decoder_encoder.1.mlp.fc2.weight', 'decoder_encoder.1.mlp.fc2.bias', 'decoder_encoder.2.norm1.weight', 'decoder_encoder.2.norm1.bias', 'decoder_encoder.2.attn.qkv.weight', 'decoder_encoder.2.attn.qkv.bias', 'decoder_encoder.2.attn.proj.weight', 'decoder_encoder.2.attn.proj.bias', 'decoder_encoder.2.norm2.weight', 'decoder_encoder.2.norm2.bias', 'decoder_encoder.2.mlp.fc1.weight', 'decoder_encoder.2.mlp.fc1.bias', 'decoder_encoder.2.mlp.fc2.weight', 'decoder_encoder.2.mlp.fc2.bias', 'decoder_encoder.3.norm1.weight', 'decoder_encoder.3.norm1.bias', 'decoder_encoder.3.attn.qkv.weight', 'decoder_encoder.3.attn.qkv.bias', 'decoder_encoder.3.attn.proj.weight', 'decoder_encoder.3.attn.proj.bias', 'decoder_encoder.3.norm2.weight', 'decoder_encoder.3.norm2.bias', 'decoder_encoder.3.mlp.fc1.weight', 'decoder_encoder.3.mlp.fc1.bias', 'decoder_encoder.3.mlp.fc2.weight', 'decoder_encoder.3.mlp.fc2.bias', 'decoder_encoder.4.norm1.weight', 'decoder_encoder.4.norm1.bias', 'decoder_encoder.4.attn.qkv.weight', 'decoder_encoder.4.attn.qkv.bias', 'decoder_encoder.4.attn.proj.weight', 'decoder_encoder.4.attn.proj.bias', 'decoder_encoder.4.norm2.weight', 'decoder_encoder.4.norm2.bias', 'decoder_encoder.4.mlp.fc1.weight', 'decoder_encoder.4.mlp.fc1.bias', 'decoder_encoder.4.mlp.fc2.weight', 'decoder_encoder.4.mlp.fc2.bias', 'decoder_encoder.5.norm1.weight', 'decoder_encoder.5.norm1.bias', 'decoder_encoder.5.attn.qkv.weight', 'decoder_encoder.5.attn.qkv.bias', 'decoder_encoder.5.attn.proj.weight', 'decoder_encoder.5.attn.proj.bias', 'decoder_encoder.5.norm2.weight', 'decoder_encoder.5.norm2.bias', 'decoder_encoder.5.mlp.fc1.weight', 'decoder_encoder.5.mlp.fc1.bias', 'decoder_encoder.5.mlp.fc2.weight', 'decoder_encoder.5.mlp.fc2.bias', 'decoder_encoder.6.norm1.weight', 'decoder_encoder.6.norm1.bias', 'decoder_encoder.6.attn.qkv.weight', 'decoder_encoder.6.attn.qkv.bias', 'decoder_encoder.6.attn.proj.weight', 'decoder_encoder.6.attn.proj.bias', 'decoder_encoder.6.norm2.weight', 'decoder_encoder.6.norm2.bias', 'decoder_encoder.6.mlp.fc1.weight', 'decoder_encoder.6.mlp.fc1.bias', 'decoder_encoder.6.mlp.fc2.weight', 'decoder_encoder.6.mlp.fc2.bias', 'decoder_encoder.7.norm1.weight', 'decoder_encoder.7.norm1.bias', 'decoder_encoder.7.attn.qkv.weight', 'decoder_encoder.7.attn.qkv.bias', 'decoder_encoder.7.attn.proj.weight', 'decoder_encoder.7.attn.proj.bias', 'decoder_encoder.7.norm2.weight', 'decoder_encoder.7.norm2.bias', 'decoder_encoder.7.mlp.fc1.weight', 'decoder_encoder.7.mlp.fc1.bias', 'decoder_encoder.7.mlp.fc2.weight', 'decoder_encoder.7.mlp.fc2.bias'])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder.0.attn.proj.bias',\n",
       " 'encoder.0.attn.proj.weight',\n",
       " 'encoder.0.attn.qkv.bias',\n",
       " 'encoder.0.attn.qkv.weight',\n",
       " 'encoder.0.mlp.fc1.bias',\n",
       " 'encoder.0.mlp.fc1.weight',\n",
       " 'encoder.0.mlp.fc2.bias',\n",
       " 'encoder.0.mlp.fc2.weight',\n",
       " 'encoder.0.norm1.bias',\n",
       " 'encoder.0.norm1.weight',\n",
       " 'encoder.0.norm2.bias',\n",
       " 'encoder.0.norm2.weight',\n",
       " 'encoder.1.attn.proj.bias',\n",
       " 'encoder.1.attn.proj.weight',\n",
       " 'encoder.1.attn.qkv.bias',\n",
       " 'encoder.1.attn.qkv.weight',\n",
       " 'encoder.1.mlp.fc1.bias',\n",
       " 'encoder.1.mlp.fc1.weight',\n",
       " 'encoder.1.mlp.fc2.bias',\n",
       " 'encoder.1.mlp.fc2.weight',\n",
       " 'encoder.1.norm1.bias',\n",
       " 'encoder.1.norm1.weight',\n",
       " 'encoder.1.norm2.bias',\n",
       " 'encoder.1.norm2.weight',\n",
       " 'encoder.10.attn.proj.bias',\n",
       " 'encoder.10.attn.proj.weight',\n",
       " 'encoder.10.attn.qkv.bias',\n",
       " 'encoder.10.attn.qkv.weight',\n",
       " 'encoder.10.mlp.fc1.bias',\n",
       " 'encoder.10.mlp.fc1.weight',\n",
       " 'encoder.10.mlp.fc2.bias',\n",
       " 'encoder.10.mlp.fc2.weight',\n",
       " 'encoder.10.norm1.bias',\n",
       " 'encoder.10.norm1.weight',\n",
       " 'encoder.10.norm2.bias',\n",
       " 'encoder.10.norm2.weight',\n",
       " 'encoder.11.attn.proj.bias',\n",
       " 'encoder.11.attn.proj.weight',\n",
       " 'encoder.11.attn.qkv.bias',\n",
       " 'encoder.11.attn.qkv.weight',\n",
       " 'encoder.11.mlp.fc1.bias',\n",
       " 'encoder.11.mlp.fc1.weight',\n",
       " 'encoder.11.mlp.fc2.bias',\n",
       " 'encoder.11.mlp.fc2.weight',\n",
       " 'encoder.11.norm1.bias',\n",
       " 'encoder.11.norm1.weight',\n",
       " 'encoder.11.norm2.bias',\n",
       " 'encoder.11.norm2.weight',\n",
       " 'encoder.2.attn.proj.bias',\n",
       " 'encoder.2.attn.proj.weight',\n",
       " 'encoder.2.attn.qkv.bias',\n",
       " 'encoder.2.attn.qkv.weight',\n",
       " 'encoder.2.mlp.fc1.bias',\n",
       " 'encoder.2.mlp.fc1.weight',\n",
       " 'encoder.2.mlp.fc2.bias',\n",
       " 'encoder.2.mlp.fc2.weight',\n",
       " 'encoder.2.norm1.bias',\n",
       " 'encoder.2.norm1.weight',\n",
       " 'encoder.2.norm2.bias',\n",
       " 'encoder.2.norm2.weight',\n",
       " 'encoder.3.attn.proj.bias',\n",
       " 'encoder.3.attn.proj.weight',\n",
       " 'encoder.3.attn.qkv.bias',\n",
       " 'encoder.3.attn.qkv.weight',\n",
       " 'encoder.3.mlp.fc1.bias',\n",
       " 'encoder.3.mlp.fc1.weight',\n",
       " 'encoder.3.mlp.fc2.bias',\n",
       " 'encoder.3.mlp.fc2.weight',\n",
       " 'encoder.3.norm1.bias',\n",
       " 'encoder.3.norm1.weight',\n",
       " 'encoder.3.norm2.bias',\n",
       " 'encoder.3.norm2.weight',\n",
       " 'encoder.4.attn.proj.bias',\n",
       " 'encoder.4.attn.proj.weight',\n",
       " 'encoder.4.attn.qkv.bias',\n",
       " 'encoder.4.attn.qkv.weight',\n",
       " 'encoder.4.mlp.fc1.bias',\n",
       " 'encoder.4.mlp.fc1.weight',\n",
       " 'encoder.4.mlp.fc2.bias',\n",
       " 'encoder.4.mlp.fc2.weight',\n",
       " 'encoder.4.norm1.bias',\n",
       " 'encoder.4.norm1.weight',\n",
       " 'encoder.4.norm2.bias',\n",
       " 'encoder.4.norm2.weight',\n",
       " 'encoder.5.attn.proj.bias',\n",
       " 'encoder.5.attn.proj.weight',\n",
       " 'encoder.5.attn.qkv.bias',\n",
       " 'encoder.5.attn.qkv.weight',\n",
       " 'encoder.5.mlp.fc1.bias',\n",
       " 'encoder.5.mlp.fc1.weight',\n",
       " 'encoder.5.mlp.fc2.bias',\n",
       " 'encoder.5.mlp.fc2.weight',\n",
       " 'encoder.5.norm1.bias',\n",
       " 'encoder.5.norm1.weight',\n",
       " 'encoder.5.norm2.bias',\n",
       " 'encoder.5.norm2.weight',\n",
       " 'encoder.6.attn.proj.bias',\n",
       " 'encoder.6.attn.proj.weight',\n",
       " 'encoder.6.attn.qkv.bias',\n",
       " 'encoder.6.attn.qkv.weight',\n",
       " 'encoder.6.mlp.fc1.bias',\n",
       " 'encoder.6.mlp.fc1.weight',\n",
       " 'encoder.6.mlp.fc2.bias',\n",
       " 'encoder.6.mlp.fc2.weight',\n",
       " 'encoder.6.norm1.bias',\n",
       " 'encoder.6.norm1.weight',\n",
       " 'encoder.6.norm2.bias',\n",
       " 'encoder.6.norm2.weight',\n",
       " 'encoder.7.attn.proj.bias',\n",
       " 'encoder.7.attn.proj.weight',\n",
       " 'encoder.7.attn.qkv.bias',\n",
       " 'encoder.7.attn.qkv.weight',\n",
       " 'encoder.7.mlp.fc1.bias',\n",
       " 'encoder.7.mlp.fc1.weight',\n",
       " 'encoder.7.mlp.fc2.bias',\n",
       " 'encoder.7.mlp.fc2.weight',\n",
       " 'encoder.7.norm1.bias',\n",
       " 'encoder.7.norm1.weight',\n",
       " 'encoder.7.norm2.bias',\n",
       " 'encoder.7.norm2.weight',\n",
       " 'encoder.8.attn.proj.bias',\n",
       " 'encoder.8.attn.proj.weight',\n",
       " 'encoder.8.attn.qkv.bias',\n",
       " 'encoder.8.attn.qkv.weight',\n",
       " 'encoder.8.mlp.fc1.bias',\n",
       " 'encoder.8.mlp.fc1.weight',\n",
       " 'encoder.8.mlp.fc2.bias',\n",
       " 'encoder.8.mlp.fc2.weight',\n",
       " 'encoder.8.norm1.bias',\n",
       " 'encoder.8.norm1.weight',\n",
       " 'encoder.8.norm2.bias',\n",
       " 'encoder.8.norm2.weight',\n",
       " 'encoder.9.attn.proj.bias',\n",
       " 'encoder.9.attn.proj.weight',\n",
       " 'encoder.9.attn.qkv.bias',\n",
       " 'encoder.9.attn.qkv.weight',\n",
       " 'encoder.9.mlp.fc1.bias',\n",
       " 'encoder.9.mlp.fc1.weight',\n",
       " 'encoder.9.mlp.fc2.bias',\n",
       " 'encoder.9.mlp.fc2.weight',\n",
       " 'encoder.9.norm1.bias',\n",
       " 'encoder.9.norm1.weight',\n",
       " 'encoder.9.norm2.bias',\n",
       " 'encoder.9.norm2.weight',\n",
       " 'global_tokens',\n",
       " 'input_adapters.rgb.pos_emb',\n",
       " 'input_adapters.rgb.proj.bias',\n",
       " 'input_adapters.rgb.proj.weight'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(checkpoint_model.keys()) & set(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoader instances for training and validation\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = create_model(\n",
    "        model_name,\n",
    "        input_adapters=input_adapters,\n",
    "        output_adapters=output_adapters,\n",
    "        drop_path_rate=drop_path_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiMAE(nn.Module):\n",
    "    '''\n",
    "    MultiMAE: Multi-task Multi-modal Masked Autoencoder\n",
    "    This module performs masking in its forward pass.\n",
    "    The MultiViT module defined below inherits from this module and performs a regular forward pass,\n",
    "    and should be used instead for downstream tasks\n",
    "\n",
    "\n",
    "    :param input_adapters: Dictionary of task -> input adapters\n",
    "    :param output_adapters: Optional dictionary of task -> output adapters\n",
    "\n",
    "    :param num_global_tokens: Number of additional global tokens to add (like cls tokens), default is 1\n",
    "    :param dim_tokens: Dimension of encoder tokens\n",
    "    :param depth: Depth of encoder\n",
    "    :param num_heads: Number of attention heads\n",
    "    :param mlp_ratio: MLP hidden dim ratio\n",
    "    :param qkv_bias: Set to False to disable bias\n",
    "    :param drop_rate: Dropout after MLPs and Attention\n",
    "    :param attn_drop_rate: Attention matrix drop rate\n",
    "    :param drop_path_rate: DropPath drop rate\n",
    "    :param norm_layer: Type of normalization layer\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 input_adapters: Dict[str, nn.Module],\n",
    "                 output_adapters: Optional[Dict[str, nn.Module]],\n",
    "                 num_global_tokens: int = 1,\n",
    "                 dim_tokens: int = 768,\n",
    "                 depth: int = 12,\n",
    "                 num_heads: int = 12,\n",
    "                 mlp_ratio: float = 4.0,\n",
    "                 qkv_bias: bool = True,\n",
    "                 drop_rate: float = 0.0,\n",
    "                 attn_drop_rate: float = 0.0,\n",
    "                 drop_path_rate: float = 0.0,\n",
    "                 norm_layer: nn.Module = partial(nn.LayerNorm, eps=1e-6)):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize input and output adapters\n",
    "        for adapter in input_adapters.values():\n",
    "            adapter.init(dim_tokens=dim_tokens)\n",
    "        self.input_adapters = nn.ModuleDict(input_adapters)\n",
    "        if output_adapters is not None:\n",
    "            for adapter in output_adapters.values():\n",
    "                adapter.init(dim_tokens_enc=dim_tokens)\n",
    "            self.output_adapters = nn.ModuleDict(output_adapters)\n",
    "        else:\n",
    "            self.output_adapters = None\n",
    "\n",
    "        # Additional learnable tokens that can be used by encoder to process/store global information\n",
    "        self.num_global_tokens = num_global_tokens\n",
    "        self.global_tokens = nn.Parameter(torch.zeros(1, num_global_tokens, dim_tokens))\n",
    "        trunc_normal_(self.global_tokens, std=0.02)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            Block(dim=dim_tokens, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        for name, m in self.named_modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if 'qkv' in name:\n",
    "                    # treat the weights of Q, K, V separately\n",
    "                    val = math.sqrt(6. / float(m.weight.shape[0] // 3 + m.weight.shape[1]))\n",
    "                    nn.init.uniform_(m.weight, -val, val)\n",
    "                elif 'kv' in name:\n",
    "                    # treat the weights of K, V separately\n",
    "                    val = math.sqrt(6. / float(m.weight.shape[0] // 2 + m.weight.shape[1]))\n",
    "                    nn.init.uniform_(m.weight, -val, val)\n",
    "\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if '.proj' in name:\n",
    "                    # From MAE, initialize projection like nn.Linear (instead of nn.Conv2d)\n",
    "                    w = m.weight.data\n",
    "                    nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def get_num_layers(self):\n",
    "        return len(self.encoder)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        no_wd_set = {'global_tokens'}\n",
    "\n",
    "        for task, adapter in self.input_adapters.items():\n",
    "            if hasattr(adapter, 'no_weight_decay'):\n",
    "                to_skip = adapter.no_weight_decay()\n",
    "                to_skip = set([f'input_adapters.{task}.{name}' for name in to_skip])\n",
    "                no_wd_set = no_wd_set | to_skip\n",
    "\n",
    "        for task, adapter in self.output_adapters.items():\n",
    "            if hasattr(adapter, 'no_weight_decay'):\n",
    "                to_skip = adapter.no_weight_decay()\n",
    "                to_skip = set([f'output_adapters.{task}.{name}' for name in to_skip])\n",
    "                no_wd_set = no_wd_set | to_skip\n",
    "\n",
    "        return no_wd_set\n",
    "\n",
    "    def sample_alphas(self, B: int, n_tasks: int, alphas: float = 1.0, eps: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Sample alphas for Dirichlet sampling such that tasks are first uniformly chosen and then Dirichlet sampling\n",
    "        is performed over the chosen ones.\n",
    "\n",
    "        :param B: Batch size\n",
    "        :param n_tasks: Number of input tasks\n",
    "        :param alphas: Float or list to multiply task choices {0,1} by\n",
    "        :param eps: Small constant since Dirichlet alphas need to be positive\n",
    "        \"\"\"\n",
    "        valid_task_choices = torch.Tensor([list(i) for i in itertools.product([0, 1], repeat=n_tasks)][1:])\n",
    "        rand_per_sample_choice = torch.randint(0, len(valid_task_choices), (B,))\n",
    "        alphas_tensor = torch.index_select(valid_task_choices, 0, rand_per_sample_choice)\n",
    "        alphas_tensor = alphas_tensor * torch.tensor(alphas) + eps\n",
    "        return alphas_tensor\n",
    "\n",
    "    def generate_random_masks(self,\n",
    "                            input_tokens: Dict[str, torch.Tensor],\n",
    "                            num_encoded_tokens: int,\n",
    "                            alphas: Union[float, List[float]] = 1.0,\n",
    "                            sample_tasks_uniformly: bool = False) :\n",
    "        \"\"\"\n",
    "        Sample a total of num_encoded_tokens from different tasks using Dirichlet sampling.\n",
    "\n",
    "        :param input_tokens: Dictionary of tensors to sample num_encoded_tokens from\n",
    "        :param num_encoded_tokens: Number of tokens to select\n",
    "        :param alphas: Dirichlet distribution parameter alpha. Lower alpha = harder,\n",
    "            less uniform sampling. Can be float or list of floats.\n",
    "        :param sample_tasks_uniformly: Set to True to first sample 1-n_tasks uniformly at random\n",
    "            for each sample in the batch. Dirichlet sampling is then done over selected subsets.\n",
    "        \"\"\"\n",
    "        B = list(input_tokens.values())[0].shape[0]\n",
    "        device = list(input_tokens.values())[0].device\n",
    "\n",
    "        alphas = [alphas] * len(input_tokens) if isinstance(alphas, float) else alphas\n",
    "        if sample_tasks_uniformly:\n",
    "            alphas = self.sample_alphas(B, len(input_tokens), alphas=alphas)\n",
    "            task_sampling_dist = Dirichlet(alphas).sample().to(device)\n",
    "        else:\n",
    "            task_sampling_dist = Dirichlet(torch.Tensor(alphas)).sample((B,)).to(device)\n",
    "\n",
    "        samples_per_task = (task_sampling_dist * num_encoded_tokens).round().long()\n",
    "\n",
    "        task_masks = []\n",
    "        num_tokens_per_task = [task_tokens.shape[1] for task_tokens in input_tokens.values()]\n",
    "        for i, num_tokens in enumerate(num_tokens_per_task):\n",
    "            # Use noise to shuffle arange\n",
    "            noise = torch.rand(B, num_tokens, device=device)  # noise in [0, 1]\n",
    "            ids_arange_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "            mask = torch.arange(num_tokens, device=device).unsqueeze(0).expand(B, -1)\n",
    "            mask = torch.gather(mask, dim=1, index=ids_arange_shuffle)\n",
    "            # 0 is keep (unmasked), 1 is remove (masked)\n",
    "            mask = torch.where(mask < samples_per_task[:, i].unsqueeze(1), 0, 1)\n",
    "            task_masks.append(mask)\n",
    "\n",
    "        mask_all = torch.cat(task_masks, dim=1)\n",
    "        ids_shuffle = torch.argsort(mask_all + torch.rand_like(mask_all.float()), dim=1)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "        ids_keep = ids_shuffle[:, :num_encoded_tokens]\n",
    "\n",
    "        # Update binary mask to adjust for task rounding\n",
    "        mask_all = torch.ones_like(mask_all)\n",
    "        mask_all[:, :num_encoded_tokens] = 0\n",
    "        # Unshuffle to get the binary mask\n",
    "        mask_all = torch.gather(mask_all, dim=1, index=ids_restore)\n",
    "        # Split to get task masks\n",
    "        task_masks = torch.split(mask_all, num_tokens_per_task, dim=1)\n",
    "        # Convert to dict\n",
    "        task_masks = {domain: mask for domain, mask in zip(input_tokens.keys(), task_masks)}\n",
    "\n",
    "        return task_masks, ids_keep, ids_restore\n",
    "\n",
    "    @staticmethod\n",
    "    def make_mask(N_H, N_W, xy_idxs, full_tasks=[], indicate_visible=True, flatten=True, device='cuda'):\n",
    "        \"\"\"\n",
    "        Creates masks for each task, given lists of un-masked x,y coordinates.\n",
    "        \"\"\"\n",
    "        xy_idxs = {\n",
    "            k: torch.LongTensor(v)\n",
    "            for k, v in xy_idxs.items()\n",
    "        }\n",
    "\n",
    "        task_masks = {\n",
    "            k: torch.ones(N_H, N_W).to(device)\n",
    "            for k in xy_idxs.keys()\n",
    "        }\n",
    "\n",
    "        for k in xy_idxs.keys():\n",
    "            if len(xy_idxs[k]) > 0:\n",
    "                task_masks[k][xy_idxs[k][:, 1], xy_idxs[k][:, 0]] = 0\n",
    "\n",
    "        for task in full_tasks:\n",
    "            task_masks[task][:] = 0\n",
    "\n",
    "        if not indicate_visible:\n",
    "            task_masks = {k: 1 - v for k, v in task_masks.items()}\n",
    "\n",
    "        if flatten:\n",
    "            task_masks = {k: v.flatten().unsqueeze(0) for k, v in task_masks.items()}\n",
    "\n",
    "        return task_masks\n",
    "\n",
    "    def generate_input_info(self, input_task_tokens, image_size):\n",
    "        input_info = OrderedDict()\n",
    "        i = 0\n",
    "        input_info['tasks'] = {}\n",
    "        for domain, tensor in input_task_tokens.items():\n",
    "            num_tokens = tensor.shape[1]\n",
    "            d = {\n",
    "                'num_tokens': num_tokens,\n",
    "                'has_2d_posemb': True,  # TODO: Modify when adding non-2D tasks\n",
    "                'start_idx': i,\n",
    "                'end_idx': i + num_tokens,\n",
    "            }\n",
    "            i += num_tokens\n",
    "            input_info['tasks'][domain] = d\n",
    "\n",
    "        input_info['image_size'] = image_size\n",
    "        input_info['num_task_tokens'] = i\n",
    "        input_info['num_global_tokens'] = self.num_global_tokens\n",
    "\n",
    "        return input_info\n",
    "\n",
    "    def forward(self, \n",
    "                x: Union[Dict[str, torch.Tensor], torch.Tensor], \n",
    "                mask_inputs: bool = True,\n",
    "                task_masks: Dict[str, torch.Tensor] = None,\n",
    "                num_encoded_tokens: int = 128,\n",
    "                alphas: Union[float, List[float]] = 1.0,\n",
    "                sample_tasks_uniformly: bool = False,\n",
    "                fp32_output_adapters: List[str] = []):\n",
    "        \"\"\"\n",
    "        Forward pass through input adapters, transformer encoder and output adapters.\n",
    "        If specified, will randomly drop input tokens.\n",
    "\n",
    "        :param x: Input tensor or dictionary of tensors\n",
    "        :param mask_inputs: Set to True to enable random masking of input patches\n",
    "        :param task_masks: Optional dictionary of task->mask pairs.\n",
    "        :param num_encoded_tokens: Number of tokens to randomly select for encoder.\n",
    "            Only used if mask_inputs is True.\n",
    "        :param alphas: Dirichlet distribution parameter alpha for task sampling.\n",
    "            Higher alpha = harder, less uniform sampling. Can be float or list of floats.\n",
    "        :param sample_tasks_uniformly: Set to True if tasks should be uniformly presampled,\n",
    "            before Dirichlet sampling decides share of masked tokens between them.\n",
    "        :param fp32_output_adapters: List of task identifiers to force output adapters to\n",
    "            run with mixed precision turned off for stability reasons.\n",
    "        \"\"\"\n",
    "\n",
    "        ## Processing input modalities\n",
    "        # If input x is a Tensor, assume it's RGB\n",
    "        x = {'rgb': x} if isinstance(x, torch.Tensor) else x\n",
    "\n",
    "        # Need image size for tokens->image reconstruction\n",
    "        # We assume that at least one of rgb or semseg is given as input before masking\n",
    "        if 'rgb' in x:\n",
    "            B, C, H, W = x['rgb'].shape\n",
    "        elif 'semseg' in x:\n",
    "            B, H, W = x['semseg'].shape\n",
    "            H *= self.input_adapters['semseg'].stride_level\n",
    "            W *= self.input_adapters['semseg'].stride_level\n",
    "        else:\n",
    "            B, C, H, W = list(x.values())[0].shape  # TODO: Deal with case where not all have same shape\n",
    "\n",
    "        # Encode selected inputs to tokens\n",
    "        input_task_tokens = {\n",
    "            domain: self.input_adapters[domain](tensor)\n",
    "            for domain, tensor in x.items()\n",
    "            if domain in self.input_adapters\n",
    "        }\n",
    "\n",
    "        input_info = self.generate_input_info(input_task_tokens=input_task_tokens, image_size=(H, W))\n",
    "\n",
    "        # Select random subset of tokens from the chosen input tasks and concatenate them\n",
    "        if mask_inputs:\n",
    "            num_encoded_tokens = num_encoded_tokens if num_encoded_tokens is not None else self.num_encoded_tokens\n",
    "        else:\n",
    "            num_encoded_tokens = sum([tensor.shape[1] for tensor in input_task_tokens.values()])\n",
    "\n",
    "        ## Generating masks\n",
    "        if task_masks is None:\n",
    "            task_masks, ids_keep, ids_restore = self.generate_random_masks(\n",
    "                input_task_tokens,\n",
    "                num_encoded_tokens,\n",
    "                alphas=alphas,\n",
    "                sample_tasks_uniformly=sample_tasks_uniformly\n",
    "            )\n",
    "        else:\n",
    "            mask_all = torch.cat([task_masks[task] for task in input_task_tokens.keys()], dim=1)\n",
    "            ids_shuffle = torch.argsort(mask_all, dim=1)\n",
    "            ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "            ids_keep = ids_shuffle[:, :(mask_all == 0).sum()]\n",
    "\n",
    "        input_tokens = torch.cat([task_tokens for task_tokens in input_task_tokens.values()], dim=1)\n",
    "\n",
    "        # Apply mask\n",
    "        input_tokens = torch.gather(input_tokens, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, input_tokens.shape[2]))\n",
    "\n",
    "        # Add global tokens to input tokens\n",
    "        global_tokens = repeat(self.global_tokens, '() n d -> b n d', b=B)\n",
    "        input_tokens = torch.cat([input_tokens, global_tokens], dim=1)\n",
    "\n",
    "        ## Transformer forward pass\n",
    "        encoder_tokens = self.encoder(input_tokens)\n",
    "\n",
    "        ## Output decoders\n",
    "        if self.output_adapters is None:\n",
    "            return encoder_tokens, task_masks\n",
    "\n",
    "        # Decode tokens for each task using task-specific output adapters\n",
    "        preds = {\n",
    "            domain: self.output_adapters[domain](\n",
    "                encoder_tokens=encoder_tokens,\n",
    "                input_info=input_info,\n",
    "                ids_keep=ids_keep,\n",
    "                ids_restore=ids_restore,\n",
    "            )\n",
    "            for domain in self.output_adapters\n",
    "            if domain not in fp32_output_adapters\n",
    "        }\n",
    "        # Force running selected output adapters in fp32 mode\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            for domain in fp32_output_adapters:\n",
    "                if domain not in self.output_adapters:\n",
    "                    continue\n",
    "                preds[domain] = self.output_adapters[domain](\n",
    "                    encoder_tokens=encoder_tokens.float(),\n",
    "                    input_info=input_info,\n",
    "                    ids_keep=ids_keep,\n",
    "                    ids_restore=ids_restore,\n",
    "                )\n",
    "        \n",
    "        return preds, task_masks\n",
    "\n",
    "\n",
    "class MultiViT(MultiMAE):\n",
    "    \"\"\"\n",
    "    MultiViT: Multi-modal Vision Transformer\n",
    "    This is MultiMAE without masking and with a simplified / faster forward pass\n",
    "\n",
    "\n",
    "    :param input_adapters: Dictionary of task -> input adapters\n",
    "    :param output_adapters: Optional dictionary of task -> output adapters\n",
    "\n",
    "    :param num_global_tokens: Number of additional global tokens to add (like cls tokens), default is 1\n",
    "    :param dim_tokens: Dimension of encoder tokens\n",
    "    :param depth: Depth of encoder\n",
    "    :param num_heads: Number of attention heads\n",
    "    :param mlp_ratio: MLP hidden dim ratio\n",
    "    :param qkv_bias: Set to False to disable bias\n",
    "    :param drop_rate: Dropout after MLPs and Attention\n",
    "    :param attn_drop_rate: Attention matrix drop rate\n",
    "    :param drop_path_rate: DropPath drop rate\n",
    "    :param norm_layer: Type of normalization layer\n",
    "    \"\"\"\n",
    "\n",
    "    def process_input(self, x):\n",
    "\n",
    "        # If input x is a Tensor, assume it's RGB\n",
    "        x = {'rgb': x} if isinstance(x, torch.Tensor) else x\n",
    "        # Need image size for tokens->image reconstruction\n",
    "        if 'rgb' in x:\n",
    "            B, _, H, W = x['rgb'].shape\n",
    "        elif 'semseg' in x:\n",
    "            B, H, W = x['semseg'].shape\n",
    "            H *= self.input_adapters['semseg'].stride_level\n",
    "            W *= self.input_adapters['semseg'].stride_level\n",
    "        else:\n",
    "            B, _, H, W = list(x.values())[0].shape  # TODO: Deal with case where not all have same shape\n",
    "\n",
    "        # Encode selected inputs to tokens\n",
    "        input_task_tokens = {\n",
    "            domain: self.input_adapters[domain](tensor)\n",
    "            for domain, tensor in x.items()\n",
    "            if domain in self.input_adapters\n",
    "        }\n",
    "\n",
    "        input_info = self.generate_input_info(input_task_tokens=input_task_tokens, image_size=(H, W))\n",
    "        input_tokens = torch.cat([task_tokens for task_tokens in input_task_tokens.values()], dim=1)\n",
    "\n",
    "        # Add global tokens to input tokens\n",
    "        global_tokens = repeat(self.global_tokens, '() n d -> b n d', b=B)\n",
    "        input_tokens = torch.cat([input_tokens, global_tokens], dim=1)\n",
    "\n",
    "        return input_tokens, input_info\n",
    "\n",
    "    def forward(self, x: Union[Dict[str, torch.Tensor], torch.Tensor], return_all_layers=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass through input adapters, transformer encoder and output adapters.\n",
    "\n",
    "        :param x: Input tensor or dictionary of tensors\n",
    "        :param return_all_layers: Set to True to return all transformer layers\n",
    "        \"\"\"\n",
    "\n",
    "        input_tokens, input_info = self.process_input(x)\n",
    "\n",
    "        # Pass tokens through Transformer\n",
    "        if not return_all_layers:\n",
    "            encoder_tokens = self.encoder(input_tokens)\n",
    "        else:\n",
    "            # Optionally access every intermediate layer\n",
    "            encoder_tokens = []\n",
    "            tokens = input_tokens\n",
    "            for block in self.encoder:\n",
    "                tokens = block(tokens)\n",
    "                encoder_tokens.append(tokens)\n",
    "\n",
    "        if self.output_adapters is None:\n",
    "            return encoder_tokens\n",
    "\n",
    "        # Decode tokens for each task using task-specific output adapters\n",
    "        preds = {\n",
    "            domain: self.output_adapters[domain](\n",
    "                encoder_tokens=encoder_tokens,\n",
    "                input_info=input_info,\n",
    "            )\n",
    "            for domain in self.output_adapters\n",
    "        }\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "def multivit_base(\n",
    "        input_adapters: Dict[str, nn.Module],\n",
    "        output_adapters: Optional[Dict[str, nn.Module]],\n",
    "        **kwargs):\n",
    "    model = MultiViT(\n",
    "        input_adapters=input_adapters,\n",
    "        output_adapters=output_adapters,\n",
    "        dim_tokens=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def rearrange_directories(root_path, subset):\n",
    "    source_rgb = os.path.join(root_path, 'rgb', 'clevr_complex', subset)\n",
    "    source_depth = os.path.join(root_path, 'depth_euclidean', 'clevr_complex', subset)\n",
    "\n",
    "    destination_root = os.path.join(root_path, 'clevr_complex', subset)\n",
    "    destination_rgb = os.path.join(destination_root, 'rgb')\n",
    "    destination_depth = os.path.join(destination_root, 'depth_euclidean')\n",
    "\n",
    "    # Create destination directories if they don't exist\n",
    "    os.makedirs(destination_root, exist_ok=True)\n",
    "    os.makedirs(destination_rgb, exist_ok=True)\n",
    "    os.makedirs(destination_depth, exist_ok=True)\n",
    "\n",
    "    # Move and rename directories\n",
    "    shutil.move(source_rgb, destination_rgb)\n",
    "    shutil.move(source_depth, destination_depth)\n",
    "\n",
    "# root = './data'\n",
    "# subset = 'test'\n",
    "# rearrange_directories(root, subset)\n",
    "\n",
    "\n",
    "def rename(root_dir):\n",
    "    import os\n",
    "    import re\n",
    "\n",
    "    \n",
    "    splits = [\"train\", \"val\", \"test\"]\n",
    "    domains = [\"rgb\", \"depth_euclidean\"]\n",
    "\n",
    "    for split in splits:\n",
    "        for domain in domains:\n",
    "            dir_path = os.path.join(root_dir, split, domain)\n",
    "\n",
    "            for filename in os.listdir(dir_path):\n",
    "                match = re.match(r'point_(\\d+)_view_(\\d+)_domain_(.+)\\.png', filename)\n",
    "                if match:\n",
    "                    new_name = f\"{match.group(1)}_{match.group(2)}.png\"\n",
    "                    old_path = os.path.join(dir_path, filename)\n",
    "                    new_path = os.path.join(dir_path, new_name)\n",
    "\n",
    "                    os.rename(old_path, new_path)\n",
    "\n",
    "    print(\"Files renamed successfully.\")\n",
    "\n",
    "# root_dir = './data/clevr_complex/'\n",
    "# rename(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
